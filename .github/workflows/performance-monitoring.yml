name: ðŸ“Š Performance Monitoring

on:
  schedule:
    # Run performance tests daily at 6 AM UTC
    - cron: '0 6 * * *'
  push:
    branches: [ main ]
    paths:
      - 'scripts/enhanced_benchmark.py'
      - 'scripts/quality_gates.py'
  workflow_dispatch:
    inputs:
      benchmark_model:
        description: 'Model path to benchmark'
        required: false
        default: 'models/demo-qlora-model'
        type: string
      test_data:
        description: 'Test data path'
        required: false
        default: 'data/test_data.jsonl'
        type: string

jobs:
  performance-benchmark:
    name: ðŸ“ˆ Performance Benchmark
    runs-on: ubuntu-latest
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: ðŸ“¦ Cache Dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            ~/.cache/huggingface
          key: performance-${{ hashFiles('**/requirements*.txt') }}
          
      - name: ðŸ”§ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-instructlab.txt
          
      - name: ðŸ“Š System Information
        run: |
          echo "## ðŸ–¥ï¸ System Information" >> $GITHUB_STEP_SUMMARY
          echo "- OS: $(uname -a)" >> $GITHUB_STEP_SUMMARY
          echo "- Python: $(python --version)" >> $GITHUB_STEP_SUMMARY
          echo "- CPU: $(nproc) cores" >> $GITHUB_STEP_SUMMARY
          echo "- Memory: $(free -h | grep '^Mem:' | awk '{print $2}')" >> $GITHUB_STEP_SUMMARY
          echo "- Disk: $(df -h / | tail -1 | awk '{print $4}') available" >> $GITHUB_STEP_SUMMARY
          
      - name: âœ… System Validation
        id: validation
        run: |
          echo "âœ… Running system validation..."
          python validate_system.py > validation_output.txt 2>&1 || true
          
          # Extract validation results
          if grep -q "SUCCESS" validation_output.txt; then
            echo "validation_status=âœ… PASSED" >> $GITHUB_OUTPUT
          else
            echo "validation_status=âš ï¸ PARTIAL" >> $GITHUB_OUTPUT
          fi
          
          cat validation_output.txt
          
      - name: ðŸ“ˆ Run Performance Benchmarks
        id: benchmark
        run: |
          echo "ðŸ“ˆ Running performance benchmarks..."
          
          # Create results directory
          mkdir -p results/performance
          
          # Run enhanced benchmark
          python scripts/enhanced_benchmark.py \
            --model ${{ github.event.inputs.benchmark_model || 'models/demo-qlora-model' }} \
            --test-data ${{ github.event.inputs.test_data || 'data/test_data.jsonl' }} \
            --output results/performance/benchmark_$(date +%Y%m%d_%H%M%S).json \
            > benchmark_output.txt 2>&1 || true
          
          # Extract key metrics
          if [ -f results/performance/benchmark_*.json ]; then
            echo "benchmark_status=âœ… COMPLETED" >> $GITHUB_OUTPUT
          else
            echo "benchmark_status=âŒ FAILED" >> $GITHUB_OUTPUT
          fi
          
          cat benchmark_output.txt
          
      - name: ðŸŽ¯ Quality Gates Check
        id: quality_gates
        run: |
          echo "ðŸŽ¯ Running quality gates check..."
          
          python scripts/quality_gates.py --stage evaluation > quality_output.txt 2>&1 || true
          
          # Check quality gate results
          if grep -q "PASSED" quality_output.txt; then
            echo "quality_status=âœ… PASSED" >> $GITHUB_OUTPUT
          else
            echo "quality_status=âŒ FAILED" >> $GITHUB_OUTPUT
          fi
          
          cat quality_output.txt
          
      - name: ðŸ“Š Generate Performance Report
        run: |
          echo "ðŸ“Š Generating performance report..."
          
          cat > performance_report.md << 'EOF'
          # ðŸ“Š QLORAX Performance Report
          
          **Date**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')  
          **Commit**: ${{ github.sha }}  
          **Branch**: ${{ github.ref_name }}  
          
          ## ðŸ–¥ï¸ Environment
          - **OS**: $(uname -s)
          - **Python**: $(python --version | cut -d' ' -f2)
          - **CPU Cores**: $(nproc)
          - **Available Memory**: $(free -h | grep '^Mem:' | awk '{print $2}')
          
          ## âœ… System Validation
          **Status**: ${{ steps.validation.outputs.validation_status }}
          
          \`\`\`
          $(cat validation_output.txt | head -20)
          \`\`\`
          
          ## ðŸ“ˆ Benchmark Results
          **Status**: ${{ steps.benchmark.outputs.benchmark_status }}
          
          \`\`\`
          $(cat benchmark_output.txt | tail -10)
          \`\`\`
          
          ## ðŸŽ¯ Quality Gates
          **Status**: ${{ steps.quality_gates.outputs.quality_status }}
          
          \`\`\`
          $(cat quality_output.txt | tail -5)
          \`\`\`
          
          ## ðŸ“Š Historical Trends
          
          | Date | Validation | Benchmark | Quality Gates |
          |------|------------|-----------|---------------|
          | $(date +%Y-%m-%d) | ${{ steps.validation.outputs.validation_status }} | ${{ steps.benchmark.outputs.benchmark_status }} | ${{ steps.quality_gates.outputs.quality_status }} |
          
          EOF
          
      - name: ðŸ“¤ Upload Performance Results
        uses: actions/upload-artifact@v3
        with:
          name: performance-report-${{ github.run_number }}
          path: |
            performance_report.md
            results/performance/
            validation_output.txt
            benchmark_output.txt
            quality_output.txt
            
      - name: ðŸ“‹ Add to Job Summary
        run: |
          cat performance_report.md >> $GITHUB_STEP_SUMMARY
          
      - name: ðŸš¨ Alert on Performance Degradation
        if: steps.quality_gates.outputs.quality_status == 'âŒ FAILED'
        run: |
          echo "ðŸš¨ Performance degradation detected!"
          echo "Quality gates failed. Please review the performance report."
          
          # In production, you would send alerts to Slack, email, etc.
          echo "::error::Quality gates failed - performance degradation detected"

  trend-analysis:
    name: ðŸ“ˆ Trend Analysis
    runs-on: ubuntu-latest
    needs: performance-benchmark
    if: always()
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        
      - name: ðŸ“Š Download Performance Data
        uses: actions/download-artifact@v3
        with:
          name: performance-report-${{ github.run_number }}
          path: ./performance-data/
          
      - name: ðŸ“ˆ Generate Trend Analysis
        run: |
          echo "ðŸ“ˆ Generating trend analysis..."
          
          # Create trend analysis report
          cat > trend_analysis.md << 'EOF'
          # ðŸ“ˆ Performance Trend Analysis
          
          ## ðŸ“Š Recent Performance History
          
          This section would contain:
          - Performance metrics over time
          - Regression detection
          - Improvement tracking
          - Comparative analysis
          
          ## ðŸŽ¯ Recommendations
          
          Based on performance trends:
          - Monitor memory usage patterns
          - Track inference speed changes
          - Watch for accuracy regressions
          - Optimize bottleneck areas
          
          ## ðŸ“‹ Next Actions
          
          - [ ] Review performance bottlenecks
          - [ ] Update performance baselines
          - [ ] Investigate any regressions
          - [ ] Plan optimization work
          
          EOF
          
          echo "ðŸ“Š Trend analysis completed"
          
      - name: ðŸ“¤ Upload Trend Analysis
        uses: actions/upload-artifact@v3
        with:
          name: trend-analysis-${{ github.run_number }}
          path: trend_analysis.md